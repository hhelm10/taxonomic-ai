{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from taxi import utils, taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from graspologic.embed import ClassicalMDS\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel,\n",
    "    PeftConfig\n",
    ")\n",
    "\n",
    "EMBEDDING_MODEL = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True, device='cuda:0')\n",
    "\n",
    "model_id = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.padding_token = tokenizer.eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side='right'\n",
    "\n",
    "\n",
    "# group 1: Health (2), Sports (5), Family (8)\n",
    "# group 2: Politics (9), business (6), society and culture (0)\n",
    "# group 3: Computers (4), Entertainment (7), Education (3) \n",
    "coarse_to_fine_dict = {\n",
    "    0: (2,5,8),\n",
    "    1: (9,6,0),\n",
    "    2: (4,7,3)\n",
    "}\n",
    "\n",
    "dataset_name = 'community-datasets/yahoo_answers_topics'\n",
    "topic_key = 'topic'\n",
    "cache_dir = '~/.cache/huggingface/datasets'\n",
    "\n",
    "N = 64\n",
    "mc = 1\n",
    "\n",
    "def get_fine_pis(coarse_pi, coarse_to_fine_dict):\n",
    "    fine_topics = []\n",
    "    fine_pi = []\n",
    "    \n",
    "    for i, coarse in enumerate(coarse_to_fine_dict):\n",
    "        for fine in coarse_to_fine_dict[coarse]:\n",
    "            fine_topics.append(fine)\n",
    "            fine_pi.append(coarse_pi[coarse] / len(coarse_to_fine_dict[coarse]))\n",
    "            \n",
    "    return tuple(fine_topics), tuple(fine_pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pi_list = [(1,0,0), (0,1,0), (0,0,1), (1/3,1/3,1/3)]\n",
    "datasets = {}\n",
    "\n",
    "for true_pi in true_pi_list:\n",
    "    fine_topics, fine_pi = get_fine_pis(true_pi, coarse_to_fine_dict)\n",
    "    datasets[true_pi] = utils.get_dataset_by_pi(fine_pi, dataset_name, topic_key, fine_topics, N, cache_dir=cache_dir, seed=mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_pis = squareform(pdist(true_pi_list))\n",
    "dist_pis /= np.linalg.norm(dist_pis)\n",
    "true_geometry = ClassicalMDS(n_components=3).fit_transform(dist_pis)\n",
    "\n",
    "data_geometry = taxi.get_dataset_geometry(true_geometry, \n",
    "                                          datasets, \n",
    "                                          EMBEDDING_MODEL\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pi_list = [\n",
    "    (1/3, 1/3, 1/3),\n",
    "    (0.5, 0.25, 0.25),\n",
    "    (0.75, 0.125, 0.125),\n",
    "    (1, 0, 0)\n",
    "]\n",
    "\n",
    "query_datasets = {}\n",
    "\n",
    "for true_pi in query_pi_list:\n",
    "    fine_topics, fine_pi = get_fine_pis(true_pi, coarse_to_fine_dict)\n",
    "    query_datasets[true_pi] = utils.get_dataset_by_pi(fine_pi, dataset_name, topic_key, fine_topics, N, cache_dir=cache_dir, seed=mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1315784,\n",
       " 'topic': 5,\n",
       " 'question_title': 'who won the third place play off in usa 1994?',\n",
       " 'question_content': 'And what was the final score',\n",
       " 'best_answer': 'Sweden beat Bulgaria 4-0 in the match for 3rd place at the Rose Bowl in Los Angeles in the 1994 FIFA World Cup held in USA.  The goals were scored by Thomas Brolin, Hakan Mild, Henrick Larsson, and Kennet Andersson.  Bulgaria, though had a slightly better record of the two during the first round (2 wins/1 loss vs 1 win/2 draws).'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_datasets[(1, 0, 0)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d9d1bddd9f45c5a8ad6185b20d6354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/N_10000_lora_128_id_0_pi_000_000_100\n",
      "Extracted true_pi: (0.0, 0.0, 1.0)\n",
      "./models/N_10000_lora_128_id_0_pi_000_100_000\n",
      "Extracted true_pi: (0.0, 1.0, 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/helivan-project-generation/venvs/helivan/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/N_10000_lora_128_id_0_pi_033_033_033\n",
      "Extracted true_pi: (0.33, 0.33, 0.33)\n",
      "./models/N_10000_lora_128_id_0_pi_100_000_000\n",
      "Extracted true_pi: (1.0, 0.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "lora_matrices = {}\n",
    "for save_string in glob.glob('./models/N_10000*lora_128*'):\n",
    "    print(save_string)\n",
    "\n",
    "    # Extract true_pi from save_string\n",
    "    pi_str = save_string.split('pi_')[-1]\n",
    "    pi_values = [int(x)/100 for x in pi_str.split('_')]\n",
    "    true_pi = tuple(pi_values)\n",
    "    print(f\"Extracted true_pi: {true_pi}\")\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"cuda:0\",\n",
    "            trust_remote_code=True\n",
    "    )\n",
    "\n",
    "\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        save_string,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='cuda:0'\n",
    "    )\n",
    "\n",
    "    loras = taxi.get_lora_matrices(model, target_modules = [\"q_proj\",\"k_proj\",\"v_proj\"])\n",
    "    lora_matrices[true_pi] = loras\n",
    "\n",
    "    #clear memory\n",
    "    del model\n",
    "    del base_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "structural_geometry = taxi.get_structural_geometry(true_geometry, lora_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  ./models/N_10000_lora_128_id_0_pi_000_000_100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/helivan-project-generation/venvs/helivan/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tQuerying with:  (0.3333333333333333, 0.3333333333333333, 0.3333333333333333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/helivan-project-generation/venvs/helivan/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/helivan-project-generation/venvs/helivan/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/helivan-project-generation/venvs/helivan/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tQuerying with:  (0.5, 0.25, 0.25)\n",
      "\tQuerying with:  (0.75, 0.125, 0.125)\n",
      "\tQuerying with:  (1, 0, 0)\n",
      "Model:  ./models/N_10000_lora_128_id_0_pi_000_100_000\n",
      "\tQuerying with:  (0.3333333333333333, 0.3333333333333333, 0.3333333333333333)\n",
      "\tQuerying with:  (0.5, 0.25, 0.25)\n",
      "\tQuerying with:  (0.75, 0.125, 0.125)\n",
      "\tQuerying with:  (1, 0, 0)\n",
      "Model:  ./models/N_10000_lora_128_id_0_pi_033_033_033\n",
      "\tQuerying with:  (0.3333333333333333, 0.3333333333333333, 0.3333333333333333)\n",
      "\tQuerying with:  (0.5, 0.25, 0.25)\n",
      "\tQuerying with:  (0.75, 0.125, 0.125)\n",
      "\tQuerying with:  (1, 0, 0)\n",
      "Model:  ./models/N_10000_lora_128_id_0_pi_100_000_000\n",
      "\tQuerying with:  (0.3333333333333333, 0.3333333333333333, 0.3333333333333333)\n",
      "\tQuerying with:  (0.5, 0.25, 0.25)\n",
      "\tQuerying with:  (0.75, 0.125, 0.125)\n",
      "\tQuerying with:  (1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "func_behav = defaultdict(dict)\n",
    "for save_string in glob.glob('./models/N_10000*lora_128*'):\n",
    "    print('Model: ' , save_string)\n",
    "\n",
    "    # Extract true_pi from save_string\n",
    "    pi_str = save_string.split('pi_')[-1]\n",
    "    pi_values = [int(x)/100 for x in pi_str.split('_')]\n",
    "    true_pi = tuple(pi_values)\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"cuda:0\",\n",
    "            trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        save_string,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='cuda:0'\n",
    "    )\n",
    "\n",
    "    for query_pi, query_dataset in query_datasets.items():\n",
    "        print('\\tQuerying with: ', query_pi)\n",
    "        query_strings = [q['question_title'] for q in query_dataset]\n",
    "        responses, hidden_states = taxi.get_outputs(model, tokenizer, query_strings, max_length=32, match_n_input_tokens=True)\n",
    "        func_behav[true_pi][query_pi] = {'responses': responses, 'hidden_states': hidden_states}\n",
    "\n",
    "    del model\n",
    "    del base_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helivan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
