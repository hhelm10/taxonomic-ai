{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da365b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from huggingface_hub import login\n",
    "# api_key_path = \"/home/paperspace/api_keys.json\"\n",
    "# with open(api_key_path, 'r') as j:\n",
    "#     key = json.loads(j.read())['hf-llama']\n",
    "\n",
    "#login(token=key)\n",
    "    \n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from graspologic.embed import ClassicalMDS\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from tqdm import tqdm\n",
    "from taxi import utils, taxi\n",
    "\n",
    "EMBEDDING_MODEL = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True, device='cuda:0')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fd603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel,\n",
    "    PeftConfig\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.linalg import orthogonal_procrustes as procrustes\n",
    "\n",
    "\n",
    "model_id = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.padding_token = tokenizer.eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side='right'\n",
    "\n",
    "\n",
    "def get_fine_pis(coarse_pi, coarse_to_fine_dict):\n",
    "    fine_topics = []\n",
    "    fine_pi = []\n",
    "    \n",
    "    for i, coarse in enumerate(coarse_to_fine_dict):\n",
    "        for fine in coarse_to_fine_dict[coarse]:\n",
    "            fine_topics.append(fine)\n",
    "            fine_pi.append(coarse_pi[coarse] / len(coarse_to_fine_dict[coarse]))\n",
    "            \n",
    "    return tuple(fine_topics), tuple(fine_pi)\n",
    "\n",
    "\n",
    "def pi_to_string(pi):\n",
    "    s=\"\"\n",
    "    for c in pi[:-1]:\n",
    "        s+= str(int(100*c // 1)).zfill(3) + '_'\n",
    "        \n",
    "    s+=str(int(100 * pi[-1] // 1)).zfill(3)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc23130",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN=True\n",
    "\n",
    "# group 1: Health (2), Sports (5), Family (8)\n",
    "# group 2: Politics (9), business (6), society and culture (0)\n",
    "# group 3: Computers (4), Entertainment (7), Education (3) \n",
    "coarse_to_fine_dict = {\n",
    "    0: (2,5,8),\n",
    "    1: (9,6,0),\n",
    "    2: (4,7,3)\n",
    "}\n",
    "\n",
    "dataset_name = 'community-datasets/yahoo_answers_topics'\n",
    "topic_key = 'topic'\n",
    "\n",
    "'''\n",
    "lora_rank_list = [8, 16, 32, 128]\n",
    "N_list = [10,50,100,500,1000,5000,10000]\n",
    "n_mc=5\n",
    "'''\n",
    "lora_rank_list = [8, 32, 128]\n",
    "N_list = [10,100,1000,10000]\n",
    "n_mc=1\n",
    "\n",
    "\n",
    "cache_dir = '~/.cache/huggingface/datasets'\n",
    "for N in N_list:\n",
    "    N_str = str(N).zfill(5)\n",
    "    \n",
    "    for lora_rank in lora_rank_list:\n",
    "        print(N, lora_rank)\n",
    "                \n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_rank,  # Reduced rank for 8B model\n",
    "            lora_alpha=2*lora_rank,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\"\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        lora_rank_str = str(lora_rank).zfill(3)\n",
    "        for mc in range(n_mc):\n",
    "            datasets = {}\n",
    "            # lora_matrices = {}\n",
    "            # normed_average_hidden_states = {}\n",
    "            # responses = {}\n",
    "            \n",
    "            ### Update true_pi_list here if necessary\n",
    "            true_pi_list = [(1,0,0), (0,1,0), (0,0,1), (1/3,1/3,1/3)]\n",
    "                                                       \n",
    "            for true_pi in true_pi_list:\n",
    "                fine_topics, fine_pi = get_fine_pis(true_pi, coarse_to_fine_dict)\n",
    "                \n",
    "                save_string = f'./models/N_{N_str}_lora_{lora_rank_str}_id_{mc}_pi_{pi_to_string(true_pi)}'\n",
    "                if os.path.exists(save_string) and TRAIN:\n",
    "                    continue\n",
    "                \n",
    "                datasets[true_pi] = utils.get_dataset_by_pi(fine_pi, dataset_name, topic_key, fine_topics, N, cache_dir=cache_dir, seed=mc)\n",
    "                \n",
    "                if 'base_model' not in locals():\n",
    "                    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                            model_id,\n",
    "                            torch_dtype=torch.float16,\n",
    "                            device_map=\"cuda:0\",\n",
    "                            trust_remote_code=True\n",
    "                    )\n",
    "                \n",
    "                if TRAIN:\n",
    "                    tokenized_dataset = utils._prepare_dataset(datasets[true_pi], tokenizer, 'question_title', 'best_answer')\n",
    "                    \n",
    "                    # Prepare model for training\n",
    "                    model = prepare_model_for_kbit_training(base_model)\n",
    "                    model = get_peft_model(model, lora_config)\n",
    "                    \n",
    "                    # Initialize trainer\n",
    "                    training_args = TrainingArguments(\n",
    "                        output_dir=save_string,\n",
    "                        num_train_epochs=3,\n",
    "                        per_device_train_batch_size=4,\n",
    "                        gradient_accumulation_steps=1,\n",
    "                        learning_rate=1e-5,  \n",
    "                        fp16=True,\n",
    "                        save_strategy='no',\n",
    "                        logging_steps=10,\n",
    "                        optim=\"paged_adamw_8bit\",\n",
    "                        lr_scheduler_type=\"cosine\"\n",
    "                    )\n",
    "\n",
    "                    trainer = Trainer(\n",
    "                        model=model,\n",
    "                        args=training_args,\n",
    "                        train_dataset=tokenized_dataset,\n",
    "                        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "                    )\n",
    "\n",
    "                    # Start training\n",
    "                    trainer.train()\n",
    "                    model.save_pretrained(save_string)\n",
    "                else:\n",
    "                    # Load base model\n",
    "                    model = PeftModel.from_pretrained(\n",
    "                                base_model,\n",
    "                                save_string,\n",
    "                                torch_dtype=torch.float16,\n",
    "                                device_map='cuda:0'\n",
    "                    )\n",
    "                                \n",
    "\n",
    "                # responses[true_pi], normed_average_hidden_states[true_pi] = taxi.get_outputs(model, tokenizer, query_set, match_n_input_tokens=True)\n",
    "                # lora_matrices[true_pi] = taxi.get_lora_matrices(model)\n",
    "\n",
    "                            \n",
    "                try:\n",
    "                    del model\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # cached_data[N][lora_rank][mc]['behavioral'] = responses.copy()\n",
    "            # cached_data[N][lora_rank][mc]['functional'] = normed_average_hidden_states.copy()\n",
    "            # cached_data[N][lora_rank][mc]['structural'] = lora_matrices.copy()\n",
    "\n",
    "            \n",
    "            # pickle.dump(cached_data, open(cache_file_path, 'wb'))\n",
    "        \n",
    "try:\n",
    "    base_model.cpu()\n",
    "    del base_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab790f-cf1d-47c1-9375-3928732b5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                            model_id,\n",
    "                            torch_dtype=torch.float16,\n",
    "                            device_map=\"cuda:0\",\n",
    "                            trust_remote_code=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "572e4562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Keep this just in case you need it in a few mins / next session\n",
    "try:\n",
    "    base_model.cpu()\n",
    "    del base_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ad28b6-40aa-4433-ae9f-5afb5d9bfcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_data = pickle.load(open(cache_file_path, 'rb'))\n",
    "\n",
    "geometries = {}\n",
    "\n",
    "true_pi_list = [(1,0,0), (0,1,0), (0,0,1), (1/3,1/3,1/3)]\n",
    "dist_pis = squareform(pdist(true_pi_list))\n",
    "dist_pis /= np.linalg.norm(dist_pis)\n",
    "true_geometry = ClassicalMDS(n_components=3).fit_transform(dist_pis)\n",
    "\n",
    "query_set_size_list = [1,2,5,10,20,50,100]\n",
    "\n",
    "# complete_datasets = []\n",
    "# for pi in true_pi_list[:-1]:\n",
    "#     fine_topics, fine_pi = get_fine_pis(pi, coarse_to_fine_dict)\n",
    "#     complete_datasets.append(utils.get_dataset_by_pi(fine_pi, dataset_name, topic_key, fine_topics, 14000*3, seed=mc))\n",
    "\n",
    "for N in tqdm(N_list):\n",
    "    geometries[N] = {}\n",
    "    for lora_rank in lora_rank_list:\n",
    "        geometries[N][lora_rank] = {}\n",
    "        for mc in tqdm(range(4)):\n",
    "            datasets = {}\n",
    "            for pi in true_pi_list:\n",
    "                dataset_size_list = []\n",
    "                dataset_size = [int(N * pi_component) for pi_component in pi]\n",
    "                remainder = N - np.sum(dataset_size)\n",
    "\n",
    "                for i in np.random.choice(range(len(complete_datasets)), size=remainder):\n",
    "                    dataset_size[i] += 1\n",
    "\n",
    "                \n",
    "                datasets[pi] = utils._merge_datasets([cd.shuffle(seed=mc).select(range(dataset_size[i])) for i, cd in enumerate(complete_datasets)])\n",
    "\n",
    "            \n",
    "            geometries[N][lora_rank][mc] = {}\n",
    "            data_geometry = taxi.get_dataset_geometry(true_geometry, \n",
    "                                                      datasets, \n",
    "                                                      EMBEDDING_MODEL\n",
    "                                                     )\n",
    "            structural_geometry = taxi.get_structural_geometry(true_geometry, \n",
    "                                                               cached_data[N][lora_rank][mc]['structural']\n",
    "                                                              )\n",
    "\n",
    "            for query_set_size in query_set_size_list:\n",
    "                geometries[N][lora_rank][mc][query_set_size] = {}\n",
    "\n",
    "                not_unique_responses=True\n",
    "                attempts=0\n",
    "                while not_unique_responses:\n",
    "                    query_indices = np.random.choice(500, query_set_size, replace=False).astype(int)\n",
    "                    attempts+=1\n",
    "    \n",
    "                    responses = {}\n",
    "                    normed_average_hidden_states = {}\n",
    "    \n",
    "                    for pi in true_pi_list:\n",
    "                        normed_average_hidden_states[pi] = cached_data[N][lora_rank][mc]['functional'][pi][query_indices]\n",
    "                        responses[pi] = np.array(cached_data[N][lora_rank][mc]['behavioral'][pi])[query_indices]\n",
    "\n",
    "                    try:\n",
    "                        geometries[N][lora_rank][mc][query_set_size]['behavioral'] = taxi.get_behavioral_geometry(true_geometry, \n",
    "                                                                                                          responses,\n",
    "                                                                                                          EMBEDDING_MODEL\n",
    "                                                                                                         )\n",
    "                        not_unique_responses=False\n",
    "                    except:\n",
    "                        print(N, lora_rank, mc, query_set_size, attempts)\n",
    "                \n",
    "                \n",
    "                \n",
    "                geometries[N][lora_rank][mc][query_set_size]['functional'] = taxi.get_functional_geometry(true_geometry,\n",
    "                                                                                                          normed_average_hidden_states\n",
    "                                                                                                         )\n",
    "                geometries[N][lora_rank][mc][query_set_size]['data'] = data_geometry\n",
    "                geometries[N][lora_rank][mc][query_set_size]['structural'] = structural_geometry\n",
    "                geometries[N][lora_rank][mc][query_set_size]['true'] = true_geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7eee50-0719-4ae2-881d-c86c7782ca29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c4b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Set up fig structure\n",
    "def get_random_geometry(true_geometry):\n",
    "    n, n_components=true_geometry.shape\n",
    "    vectors = np.random.random(size=(n, n_components))\n",
    "    \n",
    "    dist_matrix = squareform(pdist(vectors))\n",
    "    dist_matrix /= np.linalg.norm(dist_matrix)\n",
    "    \n",
    "    cmds = ClassicalMDS(n_components=n_components).fit_transform(dist_matrix)\n",
    "    \n",
    "    return utils._rotate_geometry(true_geometry, cmds)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "gs = fig.add_gridspec(2, 4)\n",
    "\n",
    "true_pi_list = [(1,0,0), (0,1,0), (0,0,1), (1/3,1/3,1/3)]\n",
    "\n",
    "lora_rank = 64\n",
    "query_set_size=20\n",
    "N_list = [10,50,100,500,1000,5000,10000]\n",
    "n_mc=4\n",
    "mc=0\n",
    "\n",
    "alpha_dict = {50: 1/5, 500: 1/3, 5000: 1}\n",
    "marker_dict = {'data': '^', 'structural': 's', 'functional': 'v', 'behavioral': 'o'}\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "ax5 = fig.add_subplot(gs[:2, 2:4])\n",
    "ax5.set_title(r'Estimating \"true\" geometry ($\\pi^{*}$)' + f'\\n |query set| = {query_set_size}; LoRA rank = {lora_rank}', fontsize=16)\n",
    "\n",
    "#- Plot dots\n",
    "ax = [ax1,ax2,ax3,ax4]\n",
    "title_dict = {'data': 'data',\n",
    "             'structural': 'structural',\n",
    "             'functional': 'functional',\n",
    "             'behavioral': 'behavioral'\n",
    "}\n",
    "\n",
    "for i, key in enumerate(title_dict):\n",
    "    if key == 'true':\n",
    "        continue\n",
    "        \n",
    "    i -= 1\n",
    "    ax[i].scatter(true_geometry[:, 0], true_geometry[:, 1], c=true_pi_list, marker='*', s=100, alpha=alpha_dict[5000])\n",
    "    \n",
    "    for N in N_list:\n",
    "        if N not in alpha_dict:\n",
    "            continue\n",
    "\n",
    "        geometry = geometries[N][lora_rank][mc][query_set_size][key]\n",
    "\n",
    "        alpha=alpha_dict[N]\n",
    "        ax[i].scatter(geometry[:, 0], geometry[:, 1], \n",
    "                   c=true_pi_list, marker=marker_dict[key], alpha=alpha, s=40)\n",
    "\n",
    "    xlim, ylim = ax[i].get_xlim(), ax[i].get_ylim()\n",
    "    \n",
    "#     ax[row,col].scatter(100, 100, marker=marker_dict[key], c='k', alpha=alpha)\n",
    "#     ax[row,col].scatter(100, 100, marker='*', label='true geometry', c='k', s=50)\n",
    "    ax[i].set_xlim(*xlim), ax[i].set_ylim(*ylim)\n",
    "    ax[i].set_title(f'{title_dict[key]}', fontsize=16)\n",
    "    \n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_yticklabels([])\n",
    "    \n",
    "    ax[i].spines['top'].set_visible(False)\n",
    "    ax[i].spines['right'].set_visible(False)\n",
    "    \n",
    "\n",
    "#- Calculate average estimation error\n",
    "distance_dict = {key: {} for key in title_dict if key != 'true'}\n",
    "\n",
    "n_components=3\n",
    "for N in N_list:\n",
    "    for type_ in distance_dict:\n",
    "        distance_dict[type_][N] = []\n",
    "    for mc in range(n_mc):\n",
    "        # if N == 5000 and mc > 0:\n",
    "        #     continue\n",
    "\n",
    "        # if mc not in cached_geometries['true'][N][64]:\n",
    "        #     continue\n",
    "            \n",
    "        temp_true = geometries[N][lora_rank][mc][query_set_size]['true']\n",
    "        for key in title_dict:\n",
    "            if key == 'true':\n",
    "                continue\n",
    "        \n",
    "            geometry = geometries[N][lora_rank][mc][query_set_size][key]\n",
    "            distance_dict[key][N].append(np.linalg.norm(geometry[:,:n_components] - true_geometry[:,:n_components]))\n",
    "            \n",
    "#- Plot average estimation errors\n",
    "ax5.set_xticks(N_list)\n",
    "ax5.set_xlabel('Amount of training data (N)', fontsize=14)\n",
    "ax5.set_ylabel(r'$ ||\\;\\pi^{*} - \\widehat{\\pi}\\;||_{F} $', fontsize=14)\n",
    "\n",
    "for key in distance_dict:\n",
    "    mean_list = [np.mean(distance_dict[key][N]) for N in N_list]\n",
    "    std_list = [np.std(distance_dict[key][N]) / np.sqrt(n_mc) for N in N_list]\n",
    "    \n",
    "    ax5.errorbar(N_list, mean_list, std_list, label=title_dict[key], lw=3)\n",
    "    \n",
    "random_errors = [np.linalg.norm(get_random_geometry(true_geometry) - true_geometry) for i in range(1000)]\n",
    "\n",
    "ax5.axhline(y = np.mean(random_errors), c='k', lw=3, ls='--', alpha=0.5, label='random')\n",
    "    \n",
    "ax5.set_yticks([0, 0.1, 0.2, 0.3])\n",
    "ax5.set_xscale('log')\n",
    "ax5.tick_params(labelsize=14)\n",
    "ax5.legend(fontsize=12)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f7a398-46a9-48fa-8320-62a585c38980",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 64\n",
    "N=1000\n",
    "query_set_size_list = [1,2,5,10,20,50,100]\n",
    "n_mc=4\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(10,5), sharey=True, sharex=True)\n",
    "abbr_N_list = [100,1000,10000]\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    N = abbr_N_list[i]\n",
    "    ax.set_title(f'\\n N={N}; LoRA rank={lora_rank}', fontsize=16)\n",
    "    \n",
    "    #- Calculate average estimation error\n",
    "    distance_dict = {key: {} for key in title_dict if key != 'true'}\n",
    "    \n",
    "    n_components=3\n",
    "    for query_set_size in query_set_size_list:\n",
    "        for type_ in distance_dict:\n",
    "            distance_dict[type_][query_set_size] = []\n",
    "        for mc in range(n_mc):\n",
    "                \n",
    "            temp_true = geometries[N][lora_rank][mc][query_set_size]['true']\n",
    "            for key in title_dict:\n",
    "                if key == 'true':\n",
    "                    continue\n",
    "            \n",
    "                geometry = geometries[N][lora_rank][mc][query_set_size][key]\n",
    "                distance_dict[key][query_set_size].append(np.linalg.norm(geometry[:,:n_components] - true_geometry[:,:n_components]))\n",
    "                \n",
    "    #- Plot average estimation errors\n",
    "    ax.set_xticks(query_set_size_list)\n",
    "    ax.set_xlabel('Number of queries (m)', fontsize=14)\n",
    "    \n",
    "    \n",
    "    for key in distance_dict:\n",
    "        mean_list = [np.mean(distance_dict[key][query_set_size]) for query_set_size in query_set_size_list]\n",
    "        std_list = [np.std(distance_dict[key][query_set_size]) / np.sqrt(n_mc) for query_set_size in query_set_size_list]\n",
    "        \n",
    "        ax.errorbar(query_set_size_list, mean_list, std_list, label=title_dict[key], lw=3)\n",
    "        \n",
    "    random_errors = [np.linalg.norm(get_random_geometry(true_geometry) - true_geometry) for i in range(1000)]\n",
    "    \n",
    "    ax.axhline(y = np.mean(random_errors), c='k', lw=3, ls='--', alpha=0.5, label='random')\n",
    "        \n",
    "    # ax.set_yticks([0, 0.1, 0.2, 0.3])\n",
    "    ax.set_xscale('log')\n",
    "    ax.tick_params(labelsize=14)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.legend(fontsize=12)\n",
    "        ax.set_ylabel(r'$ ||\\;\\pi^{*} - \\widehat{\\pi}\\;||_{F} $', fontsize=14)\n",
    "\n",
    "fig.suptitle(r'Estimating \"true\" geometry ($\\pi^{*}$)', fontsize=18)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helivan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
