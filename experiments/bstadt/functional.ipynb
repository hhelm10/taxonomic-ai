{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import json\n",
    "\n",
    "from graspologic.embed import ClassicalMDS\n",
    "from tqdm import tqdm\n",
    "\n",
    "api_key_path = \"/home/paperspace/api_keys.json\"\n",
    "\n",
    "with open(api_key_path, 'r') as j:\n",
    "    key = json.loads(j.read())['hf-llama']\n",
    "    \n",
    "from huggingface_hub import login\n",
    "login(token=key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_by_topic(dataset_name, topic_column, target_topic, sample_size=100, seed=42):\n",
    "    \"\"\"\n",
    "    Sample rows from a HuggingFace dataset for a specific topic.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset on HuggingFace\n",
    "        topic_column (str): Name of the column containing topic information\n",
    "        target_topic (str): The topic to filter for\n",
    "        sample_size (int): Number of examples to sample (default: 100)\n",
    "        seed (int): Random seed for reproducibility (default: 42)\n",
    "    \n",
    "    Returns:\n",
    "        Dataset: A filtered and sampled dataset\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    \n",
    "    # Most datasets have a 'train' split by default\n",
    "    if isinstance(dataset, dict):\n",
    "        dataset = dataset['train']\n",
    "    \n",
    "    # Filter for the target topic\n",
    "    filtered_dataset = dataset.filter(lambda x: x[topic_column] == target_topic)\n",
    "    \n",
    "    # Sample from the filtered dataset\n",
    "    sampled_dataset = filtered_dataset.shuffle(seed=seed).select(range(min(sample_size, len(filtered_dataset))))\n",
    "    \n",
    "    return sampled_dataset\n",
    "\n",
    "def merge_datasets(datasets_list, shuffle=True, seed=42):\n",
    "    \"\"\"\n",
    "    Merge multiple datasets into one.\n",
    "    \n",
    "    Args:\n",
    "        datasets_list (list): List of datasets to merge\n",
    "        shuffle (bool): Whether to shuffle the merged dataset (default: True)\n",
    "        seed (int): Random seed for shuffling (default: 42)\n",
    "    \n",
    "    Returns:\n",
    "        Dataset: Merged dataset\n",
    "    \"\"\"\n",
    "    # Concatenate all datasets\n",
    "    merged_dataset = concatenate_datasets(datasets_list)\n",
    "    \n",
    "    # Shuffle if requested\n",
    "    if shuffle:\n",
    "        merged_dataset = merged_dataset.shuffle(seed=seed)\n",
    "    \n",
    "    return merged_dataset\n",
    "\n",
    "def get_dataset_by_pi(pi, dataset_name, topic_column, target_topics, sample_size=100, seed=42):\n",
    "    assert len(target_topics) == len(pi)\n",
    "    dataset_size = [int(sample_size * pi_component) for pi_component in pi]\n",
    "    datasets = [sample_by_topic(dataset_name, topic_column, tt, dataset_size[i], seed) for i, tt in enumerate(target_topics)]\n",
    "    \n",
    "    return merge_datasets(datasets)\n",
    "\n",
    "def pi_to_string(pi):\n",
    "    s=\"\"\n",
    "    for c in pi[:-1]:\n",
    "        s+= str(int(100*c // 1)).zfill(3) + '_'\n",
    "        \n",
    "    s+=str(int(100 * pi[-1] // 1)).zfill(3)\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, tokenizer, instruction_key, response_key, max_length=512):\n",
    "    def tokenize_function(example):\n",
    "        formatted_text = f\"<|system|>You are a helpful assistant.</s><|user|>{example['question_title']}</s><|assistant|>{example['best_answer']}</s>\"\n",
    "        \n",
    "        return tokenizer(\n",
    "            formatted_text,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "\n",
    "model_id = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.padding_token = tokenizer.eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side='right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(N, lora_rank, mc, true_pi):\n",
    "    \"\"\"\n",
    "    Load a specific trained model based on parameters.\n",
    "    \n",
    "    Args:\n",
    "        N (int): Dataset size used for training\n",
    "        lora_rank (int): LoRA rank used for training\n",
    "        mc (int): Monte Carlo iteration number\n",
    "        true_pi (tuple): Distribution tuple (e.g., (1,0,0))\n",
    "    \n",
    "    Returns:\n",
    "        model: Loaded model on CPU\n",
    "    \"\"\"\n",
    "\n",
    "    # Load base model\n",
    "    save_string = f'../models/pi_{pi_to_string(true_pi)}_N_{N}_lora_{lora_rank}_id_{mc}'\n",
    "    model = AutoModelForCausalLM.from_pretrained(save_string)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Helper function for inference (now with device management)\n",
    "def generate_response(model, tokenizer, prompt, max_length=32):\n",
    "    \"\"\"\n",
    "    Generate text using the model.\n",
    "    \n",
    "    Args:\n",
    "        model: The loaded model\n",
    "        tokenizer: The tokenizer\n",
    "        prompt (str): Input prompt\n",
    "        max_length (int): Maximum length of generated text\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "    # Calculate log likelihood\n",
    "    scores = outputs.scores\n",
    "    tokens = outputs.sequences[0][inputs['input_ids'].shape[1]:]  # Remove prompt tokens\n",
    "    log_likelihood = 0.0\n",
    "    \n",
    "    for score, token in zip(scores, tokens):\n",
    "        # Get the logits for the next token\n",
    "        logits = score[0]\n",
    "        # Get the log probability of the chosen token\n",
    "        log_prob = torch.log_softmax(logits, dim=0)[token].item()\n",
    "        log_likelihood += log_prob\n",
    "    \n",
    "    return log_likelihood                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175a39ae603140bb9b010f56fac6049a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a specific model\n",
    "N = 10000\n",
    "lora_rank = 16\n",
    "mc_id = 0\n",
    "pi = (0,0,1)\n",
    "\n",
    "model = load_model(N, lora_rank, mc_id, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/venvs/pnma/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:626: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/paperspace/venvs/pnma/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-12.157916653104621, -15.436462431764085, -25.12323538120836], [-13.789711692023047, -17.308600597285476, -25.234337347443216], [-26.5332313017625, -16.088340847008112, -12.523419387973263]]\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "augs = ['You are Ian Goodfellow. ', 'You are Yann LeCunn. ', 'You are Gilles Deleuze. ']\n",
    "prompts = ['What is machine learning?', 'What is AI?', 'What is the body withoutn organs?']\n",
    "lls = []\n",
    "for aug in augs:\n",
    "    cur_lls = [generate_response(model, tokenizer, aug + prompt) for prompt in prompts]\n",
    "    lls.append(cur_lls)\n",
    "\n",
    "print(lls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between You are Ian Goodfellow.  and You are Yann LeCunn. : 0.0009233776363544166\n",
      "Distance between You are Ian Goodfellow.  and You are Gilles Deleuze. : 0.0021072577219456434\n",
      "Distance between You are Yann LeCunn.  and You are Gilles Deleuze. : 0.0015283770626410842\n"
     ]
    }
   ],
   "source": [
    "def hellinger_distance(lls1, lls2):\n",
    "\n",
    "    probs1 = torch.exp(torch.tensor(lls1))\n",
    "    probs2 = torch.exp(torch.tensor(lls2))\n",
    "\n",
    "    return torch.sqrt(0.5 * torch.sum((torch.sqrt(probs1) - torch.sqrt(probs2))**2)).item()\n",
    "\n",
    "print(f\"Hellinger distance between {augs[0]} and {augs[1]}:\", hellinger_distance(lls[0], lls[1]))\n",
    "print(f\"Hellinger distance between {augs[0]} and {augs[2]}:\", hellinger_distance(lls[0], lls[2]))\n",
    "print(f\"Hellinger distance between {augs[1]} and {augs[2]}:\", hellinger_distance(lls[1], lls[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnma",
   "language": "python",
   "name": "pnma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
